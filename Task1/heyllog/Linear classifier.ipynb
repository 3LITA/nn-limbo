{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradients are different at (0, 0). Analytic: 0.68145, Numeric: 2.04436\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 903.169097\n",
      "Epoch 1, loss: 945.783330\n",
      "Epoch 2, loss: 897.948960\n",
      "Epoch 3, loss: 902.208291\n",
      "Epoch 4, loss: 894.720178\n",
      "Epoch 5, loss: 985.036619\n",
      "Epoch 6, loss: 903.873728\n",
      "Epoch 7, loss: 1150.862375\n",
      "Epoch 8, loss: 812.165778\n",
      "Epoch 9, loss: 744.387091\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10e881860>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.14\n",
      "Epoch 0, loss: 890.760181\n",
      "Epoch 1, loss: 764.983670\n",
      "Epoch 2, loss: 773.875989\n",
      "Epoch 3, loss: 765.402528\n",
      "Epoch 4, loss: 896.725371\n",
      "Epoch 5, loss: 762.264798\n",
      "Epoch 6, loss: 838.216977\n",
      "Epoch 7, loss: 937.829392\n",
      "Epoch 8, loss: 1025.340748\n",
      "Epoch 9, loss: 804.191601\n",
      "Epoch 10, loss: 868.987493\n",
      "Epoch 11, loss: 772.364255\n",
      "Epoch 12, loss: 883.676135\n",
      "Epoch 13, loss: 854.835761\n",
      "Epoch 14, loss: 934.344160\n",
      "Epoch 15, loss: 947.343789\n",
      "Epoch 16, loss: 890.011718\n",
      "Epoch 17, loss: 750.828299\n",
      "Epoch 18, loss: 807.458693\n",
      "Epoch 19, loss: 837.016183\n",
      "Epoch 20, loss: 785.130080\n",
      "Epoch 21, loss: 928.309787\n",
      "Epoch 22, loss: 750.586186\n",
      "Epoch 23, loss: 821.967430\n",
      "Epoch 24, loss: 753.955651\n",
      "Epoch 25, loss: 832.887913\n",
      "Epoch 26, loss: 935.561663\n",
      "Epoch 27, loss: 868.944850\n",
      "Epoch 28, loss: 924.441853\n",
      "Epoch 29, loss: 822.405673\n",
      "Epoch 30, loss: 758.378876\n",
      "Epoch 31, loss: 826.613832\n",
      "Epoch 32, loss: 776.784001\n",
      "Epoch 33, loss: 714.701783\n",
      "Epoch 34, loss: 992.904736\n",
      "Epoch 35, loss: 969.975078\n",
      "Epoch 36, loss: 819.774760\n",
      "Epoch 37, loss: 974.740352\n",
      "Epoch 38, loss: 1255.569053\n",
      "Epoch 39, loss: 859.366139\n",
      "Epoch 40, loss: 733.411860\n",
      "Epoch 41, loss: 785.508918\n",
      "Epoch 42, loss: 1017.547053\n",
      "Epoch 43, loss: 812.128242\n",
      "Epoch 44, loss: 984.851431\n",
      "Epoch 45, loss: 857.537812\n",
      "Epoch 46, loss: 954.558076\n",
      "Epoch 47, loss: 808.793535\n",
      "Epoch 48, loss: 728.750398\n",
      "Epoch 49, loss: 816.574347\n",
      "Epoch 50, loss: 949.608444\n",
      "Epoch 51, loss: 894.935065\n",
      "Epoch 52, loss: 885.795359\n",
      "Epoch 53, loss: 827.082407\n",
      "Epoch 54, loss: 770.681807\n",
      "Epoch 55, loss: 742.753680\n",
      "Epoch 56, loss: 924.299863\n",
      "Epoch 57, loss: 904.180559\n",
      "Epoch 58, loss: 880.368061\n",
      "Epoch 59, loss: 891.890467\n",
      "Epoch 60, loss: 810.020795\n",
      "Epoch 61, loss: 802.664631\n",
      "Epoch 62, loss: 805.350175\n",
      "Epoch 63, loss: 998.908077\n",
      "Epoch 64, loss: 980.679949\n",
      "Epoch 65, loss: 1118.438442\n",
      "Epoch 66, loss: 830.443398\n",
      "Epoch 67, loss: 1128.540711\n",
      "Epoch 68, loss: 1042.532856\n",
      "Epoch 69, loss: 1071.793951\n",
      "Epoch 70, loss: 898.087695\n",
      "Epoch 71, loss: 780.772146\n",
      "Epoch 72, loss: 787.412088\n",
      "Epoch 73, loss: 718.184610\n",
      "Epoch 74, loss: 821.104328\n",
      "Epoch 75, loss: 1105.590253\n",
      "Epoch 76, loss: 732.576414\n",
      "Epoch 77, loss: 990.720513\n",
      "Epoch 78, loss: 793.490770\n",
      "Epoch 79, loss: 927.627629\n",
      "Epoch 80, loss: 953.643024\n",
      "Epoch 81, loss: 841.817394\n",
      "Epoch 82, loss: 1056.583959\n",
      "Epoch 83, loss: 909.455105\n",
      "Epoch 84, loss: 792.693492\n",
      "Epoch 85, loss: 975.911988\n",
      "Epoch 86, loss: 1015.134101\n",
      "Epoch 87, loss: 810.852426\n",
      "Epoch 88, loss: 995.978688\n",
      "Epoch 89, loss: 792.612430\n",
      "Epoch 90, loss: 782.750289\n",
      "Epoch 91, loss: 804.416366\n",
      "Epoch 92, loss: 778.074898\n",
      "Epoch 93, loss: 833.246122\n",
      "Epoch 94, loss: 833.295228\n",
      "Epoch 95, loss: 974.446119\n",
      "Epoch 96, loss: 874.130492\n",
      "Epoch 97, loss: 941.910375\n",
      "Epoch 98, loss: 829.517466\n",
      "Epoch 99, loss: 749.671026\n",
      "Accuracy after training for 100 epochs:  0.182\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1058.791530\n",
      "Epoch 1, loss: 921.819561\n",
      "Epoch 2, loss: 906.009270\n",
      "Epoch 3, loss: 935.574475\n",
      "Epoch 4, loss: 810.916319\n",
      "Epoch 5, loss: 729.159001\n",
      "Epoch 6, loss: 963.584631\n",
      "Epoch 7, loss: 834.966574\n",
      "Epoch 8, loss: 885.459176\n",
      "Epoch 9, loss: 823.768559\n",
      "Epoch 10, loss: 852.002371\n",
      "Epoch 11, loss: 865.045943\n",
      "Epoch 12, loss: 680.960302\n",
      "Epoch 13, loss: 928.722605\n",
      "Epoch 14, loss: 846.861125\n",
      "Epoch 15, loss: 642.665519\n",
      "Epoch 16, loss: 991.607810\n",
      "Epoch 17, loss: 714.215841\n",
      "Epoch 18, loss: 669.237357\n",
      "Epoch 19, loss: 825.653828\n",
      "Epoch 20, loss: 785.797371\n",
      "Epoch 21, loss: 696.718140\n",
      "Epoch 22, loss: 837.881632\n",
      "Epoch 23, loss: 800.225481\n",
      "Epoch 24, loss: 939.850529\n",
      "Epoch 25, loss: 1008.514861\n",
      "Epoch 26, loss: 738.067841\n",
      "Epoch 27, loss: 863.485152\n",
      "Epoch 28, loss: 892.418829\n",
      "Epoch 29, loss: 903.290335\n",
      "Epoch 30, loss: 902.729898\n",
      "Epoch 31, loss: 1003.254446\n",
      "Epoch 32, loss: 760.474704\n",
      "Epoch 33, loss: 694.568567\n",
      "Epoch 34, loss: 1003.369395\n",
      "Epoch 35, loss: 730.938789\n",
      "Epoch 36, loss: 656.621223\n",
      "Epoch 37, loss: 914.113122\n",
      "Epoch 38, loss: 816.236276\n",
      "Epoch 39, loss: 690.735213\n",
      "Epoch 40, loss: 824.598973\n",
      "Epoch 41, loss: 714.544892\n",
      "Epoch 42, loss: 724.405219\n",
      "Epoch 43, loss: 785.076832\n",
      "Epoch 44, loss: 785.398117\n",
      "Epoch 45, loss: 818.735807\n",
      "Epoch 46, loss: 634.563956\n",
      "Epoch 47, loss: 678.582332\n",
      "Epoch 48, loss: 811.775624\n",
      "Epoch 49, loss: 855.063487\n",
      "Epoch 50, loss: 747.097207\n",
      "Epoch 51, loss: 730.158462\n",
      "Epoch 52, loss: 712.537762\n",
      "Epoch 53, loss: 657.402939\n",
      "Epoch 54, loss: 927.250400\n",
      "Epoch 55, loss: 844.575651\n",
      "Epoch 56, loss: 675.613987\n",
      "Epoch 57, loss: 990.046191\n",
      "Epoch 58, loss: 743.488591\n",
      "Epoch 59, loss: 662.811970\n",
      "Epoch 60, loss: 832.326449\n",
      "Epoch 61, loss: 787.081161\n",
      "Epoch 62, loss: 769.408080\n",
      "Epoch 63, loss: 734.978163\n",
      "Epoch 64, loss: 704.555121\n",
      "Epoch 65, loss: 630.614319\n",
      "Epoch 66, loss: 804.239840\n",
      "Epoch 67, loss: 662.233731\n",
      "Epoch 68, loss: 720.084857\n",
      "Epoch 69, loss: 866.361761\n",
      "Epoch 70, loss: 702.409703\n",
      "Epoch 71, loss: 951.806226\n",
      "Epoch 72, loss: 806.733732\n",
      "Epoch 73, loss: 705.808897\n",
      "Epoch 74, loss: 850.679476\n",
      "Epoch 75, loss: 874.784965\n",
      "Epoch 76, loss: 668.850309\n",
      "Epoch 77, loss: 701.473998\n",
      "Epoch 78, loss: 760.980602\n",
      "Epoch 79, loss: 624.797802\n",
      "Epoch 80, loss: 776.669254\n",
      "Epoch 81, loss: 818.494780\n",
      "Epoch 82, loss: 661.271694\n",
      "Epoch 83, loss: 754.220548\n",
      "Epoch 84, loss: 671.424599\n",
      "Epoch 85, loss: 805.659714\n",
      "Epoch 86, loss: 770.441126\n",
      "Epoch 87, loss: 805.466458\n",
      "Epoch 88, loss: 709.919408\n",
      "Epoch 89, loss: 725.411126\n",
      "Epoch 90, loss: 731.761397\n",
      "Epoch 91, loss: 872.224730\n",
      "Epoch 92, loss: 959.184447\n",
      "Epoch 93, loss: 822.570675\n",
      "Epoch 94, loss: 742.713322\n",
      "Epoch 95, loss: 758.929104\n",
      "Epoch 96, loss: 777.365381\n",
      "Epoch 97, loss: 867.907252\n",
      "Epoch 98, loss: 632.921388\n",
      "Epoch 99, loss: 748.638734\n",
      "Epoch 100, loss: 774.359758\n",
      "Epoch 101, loss: 854.167843\n",
      "Epoch 102, loss: 640.071143\n",
      "Epoch 103, loss: 652.260636\n",
      "Epoch 104, loss: 685.230097\n",
      "Epoch 105, loss: 904.023430\n",
      "Epoch 106, loss: 678.304638\n",
      "Epoch 107, loss: 897.144076\n",
      "Epoch 108, loss: 758.386931\n",
      "Epoch 109, loss: 667.740863\n",
      "Epoch 110, loss: 815.259933\n",
      "Epoch 111, loss: 673.444035\n",
      "Epoch 112, loss: 701.017749\n",
      "Epoch 113, loss: 887.594355\n",
      "Epoch 114, loss: 678.105471\n",
      "Epoch 115, loss: 664.003111\n",
      "Epoch 116, loss: 815.492551\n",
      "Epoch 117, loss: 941.492907\n",
      "Epoch 118, loss: 717.403038\n",
      "Epoch 119, loss: 716.299411\n",
      "Epoch 120, loss: 829.670977\n",
      "Epoch 121, loss: 717.279404\n",
      "Epoch 122, loss: 717.248114\n",
      "Epoch 123, loss: 901.123219\n",
      "Epoch 124, loss: 642.141032\n",
      "Epoch 125, loss: 800.781061\n",
      "Epoch 126, loss: 770.553875\n",
      "Epoch 127, loss: 964.850480\n",
      "Epoch 128, loss: 854.722202\n",
      "Epoch 129, loss: 811.204172\n",
      "Epoch 130, loss: 656.194434\n",
      "Epoch 131, loss: 699.659160\n",
      "Epoch 132, loss: 717.856861\n",
      "Epoch 133, loss: 717.210105\n",
      "Epoch 134, loss: 743.680663\n",
      "Epoch 135, loss: 789.975232\n",
      "Epoch 136, loss: 937.853759\n",
      "Epoch 137, loss: 670.485373\n",
      "Epoch 138, loss: 703.169601\n",
      "Epoch 139, loss: 757.668441\n",
      "Epoch 140, loss: 829.773707\n",
      "Epoch 141, loss: 678.381026\n",
      "Epoch 142, loss: 733.312362\n",
      "Epoch 143, loss: 804.953520\n",
      "Epoch 144, loss: 862.767476\n",
      "Epoch 145, loss: 753.528212\n",
      "Epoch 146, loss: 760.789405\n",
      "Epoch 147, loss: 826.426639\n",
      "Epoch 148, loss: 705.765452\n",
      "Epoch 149, loss: 660.363337\n",
      "Epoch 150, loss: 795.954668\n",
      "Epoch 151, loss: 850.207921\n",
      "Epoch 152, loss: 662.401793\n",
      "Epoch 153, loss: 795.340363\n",
      "Epoch 154, loss: 773.267062\n",
      "Epoch 155, loss: 847.535490\n",
      "Epoch 156, loss: 837.826737\n",
      "Epoch 157, loss: 697.375551\n",
      "Epoch 158, loss: 806.509127\n",
      "Epoch 159, loss: 811.766343\n",
      "Epoch 160, loss: 666.481120\n",
      "Epoch 161, loss: 799.669560\n",
      "Epoch 162, loss: 705.621924\n",
      "Epoch 163, loss: 717.671237\n",
      "Epoch 164, loss: 789.588854\n",
      "Epoch 165, loss: 778.552037\n",
      "Epoch 166, loss: 689.726701\n",
      "Epoch 167, loss: 737.054671\n",
      "Epoch 168, loss: 918.667451\n",
      "Epoch 169, loss: 781.475158\n",
      "Epoch 170, loss: 677.678039\n",
      "Epoch 171, loss: 687.493866\n",
      "Epoch 172, loss: 623.595179\n",
      "Epoch 173, loss: 771.245868\n",
      "Epoch 174, loss: 878.250170\n",
      "Epoch 175, loss: 691.253985\n",
      "Epoch 176, loss: 650.731521\n",
      "Epoch 177, loss: 715.383662\n",
      "Epoch 178, loss: 747.977226\n",
      "Epoch 179, loss: 814.639337\n",
      "Epoch 180, loss: 766.611600\n",
      "Epoch 181, loss: 892.653110\n",
      "Epoch 182, loss: 715.546386\n",
      "Epoch 183, loss: 691.610033\n",
      "Epoch 184, loss: 754.073807\n",
      "Epoch 185, loss: 691.718999\n",
      "Epoch 186, loss: 748.739169\n",
      "Epoch 187, loss: 809.408449\n",
      "Epoch 188, loss: 717.202692\n",
      "Epoch 189, loss: 650.708712\n",
      "Epoch 190, loss: 756.625025\n",
      "Epoch 191, loss: 735.127223\n",
      "Epoch 192, loss: 992.126914\n",
      "Epoch 193, loss: 751.772988\n",
      "Epoch 194, loss: 770.982357\n",
      "Epoch 195, loss: 729.629770\n",
      "Epoch 196, loss: 853.776647\n",
      "Epoch 197, loss: 707.435531\n",
      "Epoch 198, loss: 865.022930\n",
      "Epoch 199, loss: 742.415603\n",
      "Epoch 0, loss: 768.906430\n",
      "Epoch 1, loss: 954.561991\n",
      "Epoch 2, loss: 763.108497\n",
      "Epoch 3, loss: 1002.959175\n",
      "Epoch 4, loss: 800.698258\n",
      "Epoch 5, loss: 809.996045\n",
      "Epoch 6, loss: 750.171257\n",
      "Epoch 7, loss: 973.092190\n",
      "Epoch 8, loss: 987.104198\n",
      "Epoch 9, loss: 830.855279\n",
      "Epoch 10, loss: 721.853805\n",
      "Epoch 11, loss: 842.273425\n",
      "Epoch 12, loss: 824.162113\n",
      "Epoch 13, loss: 709.524811\n",
      "Epoch 14, loss: 745.122981\n",
      "Epoch 15, loss: 710.807825\n",
      "Epoch 16, loss: 880.170789\n",
      "Epoch 17, loss: 764.993671\n",
      "Epoch 18, loss: 789.644652\n",
      "Epoch 19, loss: 740.398845\n",
      "Epoch 20, loss: 992.576847\n",
      "Epoch 21, loss: 866.343419\n",
      "Epoch 22, loss: 820.967580\n",
      "Epoch 23, loss: 859.371331\n",
      "Epoch 24, loss: 704.098138\n",
      "Epoch 25, loss: 826.748694\n",
      "Epoch 26, loss: 786.581779\n",
      "Epoch 27, loss: 843.727455\n",
      "Epoch 28, loss: 1032.307813\n",
      "Epoch 29, loss: 922.619803\n",
      "Epoch 30, loss: 823.419106\n",
      "Epoch 31, loss: 912.071172\n",
      "Epoch 32, loss: 1070.280059\n",
      "Epoch 33, loss: 726.401140\n",
      "Epoch 34, loss: 884.852797\n",
      "Epoch 35, loss: 838.976211\n",
      "Epoch 36, loss: 685.164823\n",
      "Epoch 37, loss: 794.820670\n",
      "Epoch 38, loss: 749.791970\n",
      "Epoch 39, loss: 685.701850\n",
      "Epoch 40, loss: 938.338125\n",
      "Epoch 41, loss: 691.342428\n",
      "Epoch 42, loss: 711.422147\n",
      "Epoch 43, loss: 897.094545\n",
      "Epoch 44, loss: 895.915889\n",
      "Epoch 45, loss: 849.528984\n",
      "Epoch 46, loss: 672.280922\n",
      "Epoch 47, loss: 793.297643\n",
      "Epoch 48, loss: 690.549217\n",
      "Epoch 49, loss: 804.652610\n",
      "Epoch 50, loss: 840.085609\n",
      "Epoch 51, loss: 883.782721\n",
      "Epoch 52, loss: 782.067940\n",
      "Epoch 53, loss: 802.871991\n",
      "Epoch 54, loss: 694.402753\n",
      "Epoch 55, loss: 791.865939\n",
      "Epoch 56, loss: 730.701372\n",
      "Epoch 57, loss: 806.860879\n",
      "Epoch 58, loss: 758.334588\n",
      "Epoch 59, loss: 825.284548\n",
      "Epoch 60, loss: 815.862024\n",
      "Epoch 61, loss: 645.594950\n",
      "Epoch 62, loss: 769.674953\n",
      "Epoch 63, loss: 706.596183\n",
      "Epoch 64, loss: 777.241682\n",
      "Epoch 65, loss: 778.515423\n",
      "Epoch 66, loss: 703.770398\n",
      "Epoch 67, loss: 708.489642\n",
      "Epoch 68, loss: 774.356742\n",
      "Epoch 69, loss: 741.516427\n",
      "Epoch 70, loss: 871.201638\n",
      "Epoch 71, loss: 701.847664\n",
      "Epoch 72, loss: 634.602704\n",
      "Epoch 73, loss: 835.212571\n",
      "Epoch 74, loss: 796.669159\n",
      "Epoch 75, loss: 748.080424\n",
      "Epoch 76, loss: 763.772610\n",
      "Epoch 77, loss: 766.783370\n",
      "Epoch 78, loss: 846.854279\n",
      "Epoch 79, loss: 696.018523\n",
      "Epoch 80, loss: 795.294384\n",
      "Epoch 81, loss: 685.365434\n",
      "Epoch 82, loss: 680.928279\n",
      "Epoch 83, loss: 684.905457\n",
      "Epoch 84, loss: 873.157040\n",
      "Epoch 85, loss: 939.551400\n",
      "Epoch 86, loss: 696.141417\n",
      "Epoch 87, loss: 885.225378\n",
      "Epoch 88, loss: 843.749296\n",
      "Epoch 89, loss: 671.913509\n",
      "Epoch 90, loss: 732.134697\n",
      "Epoch 91, loss: 772.145900\n",
      "Epoch 92, loss: 801.066849\n",
      "Epoch 93, loss: 737.205763\n",
      "Epoch 94, loss: 737.489816\n",
      "Epoch 95, loss: 809.632253\n",
      "Epoch 96, loss: 720.673732\n",
      "Epoch 97, loss: 807.744477\n",
      "Epoch 98, loss: 795.619185\n",
      "Epoch 99, loss: 755.578934\n",
      "Epoch 100, loss: 691.095395\n",
      "Epoch 101, loss: 671.026834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102, loss: 679.971559\n",
      "Epoch 103, loss: 947.715470\n",
      "Epoch 104, loss: 695.773345\n",
      "Epoch 105, loss: 705.107025\n",
      "Epoch 106, loss: 812.184457\n",
      "Epoch 107, loss: 745.433632\n",
      "Epoch 108, loss: 763.172386\n",
      "Epoch 109, loss: 787.499800\n",
      "Epoch 110, loss: 763.033451\n",
      "Epoch 111, loss: 733.158099\n",
      "Epoch 112, loss: 754.397266\n",
      "Epoch 113, loss: 823.817230\n",
      "Epoch 114, loss: 617.994838\n",
      "Epoch 115, loss: 970.749417\n",
      "Epoch 116, loss: 895.646811\n",
      "Epoch 117, loss: 833.672195\n",
      "Epoch 118, loss: 749.735366\n",
      "Epoch 119, loss: 789.519613\n",
      "Epoch 120, loss: 796.420094\n",
      "Epoch 121, loss: 712.555426\n",
      "Epoch 122, loss: 793.530306\n",
      "Epoch 123, loss: 645.367765\n",
      "Epoch 124, loss: 648.911543\n",
      "Epoch 125, loss: 764.981066\n",
      "Epoch 126, loss: 876.123639\n",
      "Epoch 127, loss: 825.573088\n",
      "Epoch 128, loss: 712.428425\n",
      "Epoch 129, loss: 882.000466\n",
      "Epoch 130, loss: 843.489554\n",
      "Epoch 131, loss: 777.154286\n",
      "Epoch 132, loss: 829.448907\n",
      "Epoch 133, loss: 777.750523\n",
      "Epoch 134, loss: 726.125272\n",
      "Epoch 135, loss: 842.594470\n",
      "Epoch 136, loss: 798.954628\n",
      "Epoch 137, loss: 794.085407\n",
      "Epoch 138, loss: 756.717984\n",
      "Epoch 139, loss: 668.007593\n",
      "Epoch 140, loss: 778.337449\n",
      "Epoch 141, loss: 786.386039\n",
      "Epoch 142, loss: 739.533579\n",
      "Epoch 143, loss: 798.258328\n",
      "Epoch 144, loss: 705.107659\n",
      "Epoch 145, loss: 808.466353\n",
      "Epoch 146, loss: 642.588319\n",
      "Epoch 147, loss: 740.667146\n",
      "Epoch 148, loss: 648.843731\n",
      "Epoch 149, loss: 673.906141\n",
      "Epoch 150, loss: 616.833746\n",
      "Epoch 151, loss: 931.202476\n",
      "Epoch 152, loss: 788.456178\n",
      "Epoch 153, loss: 935.848046\n",
      "Epoch 154, loss: 688.999613\n",
      "Epoch 155, loss: 734.479737\n",
      "Epoch 156, loss: 784.217577\n",
      "Epoch 157, loss: 792.119256\n",
      "Epoch 158, loss: 694.824817\n",
      "Epoch 159, loss: 845.739180\n",
      "Epoch 160, loss: 662.159781\n",
      "Epoch 161, loss: 680.858794\n",
      "Epoch 162, loss: 778.005412\n",
      "Epoch 163, loss: 678.816776\n",
      "Epoch 164, loss: 721.384695\n",
      "Epoch 165, loss: 637.808930\n",
      "Epoch 166, loss: 723.723612\n",
      "Epoch 167, loss: 708.117896\n",
      "Epoch 168, loss: 680.103588\n",
      "Epoch 169, loss: 872.432445\n",
      "Epoch 170, loss: 788.663354\n",
      "Epoch 171, loss: 890.909078\n",
      "Epoch 172, loss: 755.525460\n",
      "Epoch 173, loss: 709.088779\n",
      "Epoch 174, loss: 734.364535\n",
      "Epoch 175, loss: 755.293794\n",
      "Epoch 176, loss: 707.203676\n",
      "Epoch 177, loss: 727.227425\n",
      "Epoch 178, loss: 802.239150\n",
      "Epoch 179, loss: 610.142856\n",
      "Epoch 180, loss: 796.742466\n",
      "Epoch 181, loss: 719.296396\n",
      "Epoch 182, loss: 649.287513\n",
      "Epoch 183, loss: 697.421878\n",
      "Epoch 184, loss: 732.468827\n",
      "Epoch 185, loss: 674.054394\n",
      "Epoch 186, loss: 759.740770\n",
      "Epoch 187, loss: 613.552376\n",
      "Epoch 188, loss: 790.064238\n",
      "Epoch 189, loss: 727.595491\n",
      "Epoch 190, loss: 816.557278\n",
      "Epoch 191, loss: 648.184202\n",
      "Epoch 192, loss: 628.348456\n",
      "Epoch 193, loss: 716.167276\n",
      "Epoch 194, loss: 776.933848\n",
      "Epoch 195, loss: 781.881694\n",
      "Epoch 196, loss: 804.595127\n",
      "Epoch 197, loss: 798.636549\n",
      "Epoch 198, loss: 571.835668\n",
      "Epoch 199, loss: 732.065758\n",
      "Epoch 0, loss: 752.505787\n",
      "Epoch 1, loss: 941.619934\n",
      "Epoch 2, loss: 728.523356\n",
      "Epoch 3, loss: 837.269407\n",
      "Epoch 4, loss: 948.146744\n",
      "Epoch 5, loss: 767.456447\n",
      "Epoch 6, loss: 1003.056785\n",
      "Epoch 7, loss: 736.036209\n",
      "Epoch 8, loss: 1006.131040\n",
      "Epoch 9, loss: 907.849114\n",
      "Epoch 10, loss: 742.551454\n",
      "Epoch 11, loss: 993.296464\n",
      "Epoch 12, loss: 854.192708\n",
      "Epoch 13, loss: 867.052552\n",
      "Epoch 14, loss: 844.728172\n",
      "Epoch 15, loss: 1053.934827\n",
      "Epoch 16, loss: 857.717168\n",
      "Epoch 17, loss: 755.888344\n",
      "Epoch 18, loss: 785.106033\n",
      "Epoch 19, loss: 677.333519\n",
      "Epoch 20, loss: 888.725553\n",
      "Epoch 21, loss: 859.211537\n",
      "Epoch 22, loss: 881.263637\n",
      "Epoch 23, loss: 1132.010384\n",
      "Epoch 24, loss: 837.569801\n",
      "Epoch 25, loss: 873.093518\n",
      "Epoch 26, loss: 979.031014\n",
      "Epoch 27, loss: 797.392677\n",
      "Epoch 28, loss: 706.701793\n",
      "Epoch 29, loss: 718.379453\n",
      "Epoch 30, loss: 774.190603\n",
      "Epoch 31, loss: 757.454401\n",
      "Epoch 32, loss: 759.261930\n",
      "Epoch 33, loss: 751.823857\n",
      "Epoch 34, loss: 852.893436\n",
      "Epoch 35, loss: 895.457824\n",
      "Epoch 36, loss: 862.184683\n",
      "Epoch 37, loss: 787.429196\n",
      "Epoch 38, loss: 752.189404\n",
      "Epoch 39, loss: 966.268835\n",
      "Epoch 40, loss: 861.740770\n",
      "Epoch 41, loss: 762.992644\n",
      "Epoch 42, loss: 815.366011\n",
      "Epoch 43, loss: 711.294648\n",
      "Epoch 44, loss: 865.649103\n",
      "Epoch 45, loss: 719.102826\n",
      "Epoch 46, loss: 836.424837\n",
      "Epoch 47, loss: 704.122092\n",
      "Epoch 48, loss: 693.692360\n",
      "Epoch 49, loss: 724.410704\n",
      "Epoch 50, loss: 742.471273\n",
      "Epoch 51, loss: 830.083535\n",
      "Epoch 52, loss: 705.215418\n",
      "Epoch 53, loss: 748.340546\n",
      "Epoch 54, loss: 936.811243\n",
      "Epoch 55, loss: 764.954934\n",
      "Epoch 56, loss: 840.631427\n",
      "Epoch 57, loss: 788.752405\n",
      "Epoch 58, loss: 833.055887\n",
      "Epoch 59, loss: 840.802641\n",
      "Epoch 60, loss: 961.277142\n",
      "Epoch 61, loss: 726.271450\n",
      "Epoch 62, loss: 846.764937\n",
      "Epoch 63, loss: 806.177660\n",
      "Epoch 64, loss: 901.893954\n",
      "Epoch 65, loss: 651.937627\n",
      "Epoch 66, loss: 855.026955\n",
      "Epoch 67, loss: 800.875842\n",
      "Epoch 68, loss: 775.199036\n",
      "Epoch 69, loss: 764.559847\n",
      "Epoch 70, loss: 689.602252\n",
      "Epoch 71, loss: 772.205498\n",
      "Epoch 72, loss: 670.211247\n",
      "Epoch 73, loss: 714.344532\n",
      "Epoch 74, loss: 692.442785\n",
      "Epoch 75, loss: 688.963071\n",
      "Epoch 76, loss: 712.666685\n",
      "Epoch 77, loss: 853.903232\n",
      "Epoch 78, loss: 772.457868\n",
      "Epoch 79, loss: 719.196047\n",
      "Epoch 80, loss: 776.914511\n",
      "Epoch 81, loss: 804.356511\n",
      "Epoch 82, loss: 762.288156\n",
      "Epoch 83, loss: 750.755558\n",
      "Epoch 84, loss: 775.822552\n",
      "Epoch 85, loss: 691.177304\n",
      "Epoch 86, loss: 815.312164\n",
      "Epoch 87, loss: 661.408114\n",
      "Epoch 88, loss: 846.388666\n",
      "Epoch 89, loss: 706.489319\n",
      "Epoch 90, loss: 753.161514\n",
      "Epoch 91, loss: 823.043005\n",
      "Epoch 92, loss: 826.099590\n",
      "Epoch 93, loss: 901.656772\n",
      "Epoch 94, loss: 733.511555\n",
      "Epoch 95, loss: 709.063531\n",
      "Epoch 96, loss: 943.064918\n",
      "Epoch 97, loss: 751.923467\n",
      "Epoch 98, loss: 868.268354\n",
      "Epoch 99, loss: 876.147610\n",
      "Epoch 100, loss: 772.108233\n",
      "Epoch 101, loss: 796.244669\n",
      "Epoch 102, loss: 663.391453\n",
      "Epoch 103, loss: 867.915845\n",
      "Epoch 104, loss: 663.737511\n",
      "Epoch 105, loss: 687.309820\n",
      "Epoch 106, loss: 785.639593\n",
      "Epoch 107, loss: 871.672135\n",
      "Epoch 108, loss: 743.887849\n",
      "Epoch 109, loss: 796.561645\n",
      "Epoch 110, loss: 809.458802\n",
      "Epoch 111, loss: 653.089472\n",
      "Epoch 112, loss: 681.534605\n",
      "Epoch 113, loss: 734.139182\n",
      "Epoch 114, loss: 715.468724\n",
      "Epoch 115, loss: 721.446796\n",
      "Epoch 116, loss: 752.303231\n",
      "Epoch 117, loss: 647.323859\n",
      "Epoch 118, loss: 807.660087\n",
      "Epoch 119, loss: 832.905284\n",
      "Epoch 120, loss: 737.149325\n",
      "Epoch 121, loss: 651.496137\n",
      "Epoch 122, loss: 799.966769\n",
      "Epoch 123, loss: 807.313013\n",
      "Epoch 124, loss: 955.197218\n",
      "Epoch 125, loss: 990.837434\n",
      "Epoch 126, loss: 738.939082\n",
      "Epoch 127, loss: 670.956731\n",
      "Epoch 128, loss: 794.581335\n",
      "Epoch 129, loss: 885.545550\n",
      "Epoch 130, loss: 857.209662\n",
      "Epoch 131, loss: 672.407758\n",
      "Epoch 132, loss: 665.383354\n",
      "Epoch 133, loss: 727.886379\n",
      "Epoch 134, loss: 690.391740\n",
      "Epoch 135, loss: 849.469317\n",
      "Epoch 136, loss: 732.251830\n",
      "Epoch 137, loss: 724.175816\n",
      "Epoch 138, loss: 896.864333\n",
      "Epoch 139, loss: 776.767339\n",
      "Epoch 140, loss: 807.064069\n",
      "Epoch 141, loss: 639.873919\n",
      "Epoch 142, loss: 726.109035\n",
      "Epoch 143, loss: 757.399012\n",
      "Epoch 144, loss: 657.962417\n",
      "Epoch 145, loss: 721.388414\n",
      "Epoch 146, loss: 839.291130\n",
      "Epoch 147, loss: 951.537495\n",
      "Epoch 148, loss: 798.287532\n",
      "Epoch 149, loss: 771.388414\n",
      "Epoch 150, loss: 730.758382\n",
      "Epoch 151, loss: 837.343013\n",
      "Epoch 152, loss: 632.004055\n",
      "Epoch 153, loss: 739.330356\n",
      "Epoch 154, loss: 730.358312\n",
      "Epoch 155, loss: 920.853652\n",
      "Epoch 156, loss: 786.799601\n",
      "Epoch 157, loss: 696.027238\n",
      "Epoch 158, loss: 842.772018\n",
      "Epoch 159, loss: 696.369115\n",
      "Epoch 160, loss: 703.289204\n",
      "Epoch 161, loss: 698.441825\n",
      "Epoch 162, loss: 682.114957\n",
      "Epoch 163, loss: 717.002397\n",
      "Epoch 164, loss: 664.298581\n",
      "Epoch 165, loss: 624.469211\n",
      "Epoch 166, loss: 792.534818\n",
      "Epoch 167, loss: 649.387696\n",
      "Epoch 168, loss: 826.343916\n",
      "Epoch 169, loss: 724.875218\n",
      "Epoch 170, loss: 700.288369\n",
      "Epoch 171, loss: 769.758375\n",
      "Epoch 172, loss: 878.902804\n",
      "Epoch 173, loss: 735.529223\n",
      "Epoch 174, loss: 721.501459\n",
      "Epoch 175, loss: 757.036067\n",
      "Epoch 176, loss: 665.635243\n",
      "Epoch 177, loss: 813.238200\n",
      "Epoch 178, loss: 879.320565\n",
      "Epoch 179, loss: 750.114849\n",
      "Epoch 180, loss: 808.480653\n",
      "Epoch 181, loss: 643.866974\n",
      "Epoch 182, loss: 859.589825\n",
      "Epoch 183, loss: 847.678414\n",
      "Epoch 184, loss: 704.028850\n",
      "Epoch 185, loss: 838.566796\n",
      "Epoch 186, loss: 612.903178\n",
      "Epoch 187, loss: 920.355309\n",
      "Epoch 188, loss: 806.310203\n",
      "Epoch 189, loss: 617.633870\n",
      "Epoch 190, loss: 677.987488\n",
      "Epoch 191, loss: 762.709311\n",
      "Epoch 192, loss: 761.069443\n",
      "Epoch 193, loss: 846.038321\n",
      "Epoch 194, loss: 964.496540\n",
      "Epoch 195, loss: 681.796148\n",
      "Epoch 196, loss: 699.281618\n",
      "Epoch 197, loss: 616.324650\n",
      "Epoch 198, loss: 653.661079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199, loss: 856.265257\n",
      "Epoch 0, loss: 685.163770\n",
      "Epoch 1, loss: 683.986308\n",
      "Epoch 2, loss: 675.657026\n",
      "Epoch 3, loss: 669.777871\n",
      "Epoch 4, loss: 660.724661\n",
      "Epoch 5, loss: 657.218068\n",
      "Epoch 6, loss: 656.732152\n",
      "Epoch 7, loss: 660.242768\n",
      "Epoch 8, loss: 664.579868\n",
      "Epoch 9, loss: 652.195705\n",
      "Epoch 10, loss: 659.008511\n",
      "Epoch 11, loss: 658.150894\n",
      "Epoch 12, loss: 648.183066\n",
      "Epoch 13, loss: 656.414653\n",
      "Epoch 14, loss: 649.118789\n",
      "Epoch 15, loss: 648.834453\n",
      "Epoch 16, loss: 654.877575\n",
      "Epoch 17, loss: 636.631543\n",
      "Epoch 18, loss: 645.084869\n",
      "Epoch 19, loss: 654.405237\n",
      "Epoch 20, loss: 650.516745\n",
      "Epoch 21, loss: 646.817951\n",
      "Epoch 22, loss: 641.848228\n",
      "Epoch 23, loss: 649.434319\n",
      "Epoch 24, loss: 648.957964\n",
      "Epoch 25, loss: 637.331454\n",
      "Epoch 26, loss: 644.654912\n",
      "Epoch 27, loss: 647.198103\n",
      "Epoch 28, loss: 633.782498\n",
      "Epoch 29, loss: 651.229306\n",
      "Epoch 30, loss: 645.231165\n",
      "Epoch 31, loss: 651.420775\n",
      "Epoch 32, loss: 645.107934\n",
      "Epoch 33, loss: 650.582266\n",
      "Epoch 34, loss: 634.639606\n",
      "Epoch 35, loss: 641.607662\n",
      "Epoch 36, loss: 631.330032\n",
      "Epoch 37, loss: 645.049570\n",
      "Epoch 38, loss: 637.928080\n",
      "Epoch 39, loss: 619.072303\n",
      "Epoch 40, loss: 641.210840\n",
      "Epoch 41, loss: 652.515123\n",
      "Epoch 42, loss: 635.538032\n",
      "Epoch 43, loss: 631.288483\n",
      "Epoch 44, loss: 633.837956\n",
      "Epoch 45, loss: 646.130364\n",
      "Epoch 46, loss: 641.717852\n",
      "Epoch 47, loss: 623.777801\n",
      "Epoch 48, loss: 621.159937\n",
      "Epoch 49, loss: 631.911559\n",
      "Epoch 50, loss: 623.362783\n",
      "Epoch 51, loss: 631.041599\n",
      "Epoch 52, loss: 630.345915\n",
      "Epoch 53, loss: 648.049982\n",
      "Epoch 54, loss: 638.933210\n",
      "Epoch 55, loss: 641.388307\n",
      "Epoch 56, loss: 644.648987\n",
      "Epoch 57, loss: 624.872074\n",
      "Epoch 58, loss: 620.597268\n",
      "Epoch 59, loss: 631.355713\n",
      "Epoch 60, loss: 629.406538\n",
      "Epoch 61, loss: 626.596034\n",
      "Epoch 62, loss: 622.930551\n",
      "Epoch 63, loss: 617.880707\n",
      "Epoch 64, loss: 637.316557\n",
      "Epoch 65, loss: 642.358530\n",
      "Epoch 66, loss: 627.197506\n",
      "Epoch 67, loss: 623.257677\n",
      "Epoch 68, loss: 651.338350\n",
      "Epoch 69, loss: 628.877599\n",
      "Epoch 70, loss: 628.677411\n",
      "Epoch 71, loss: 626.029761\n",
      "Epoch 72, loss: 633.631026\n",
      "Epoch 73, loss: 619.470542\n",
      "Epoch 74, loss: 630.268454\n",
      "Epoch 75, loss: 645.711142\n",
      "Epoch 76, loss: 616.233062\n",
      "Epoch 77, loss: 635.156361\n",
      "Epoch 78, loss: 625.908247\n",
      "Epoch 79, loss: 622.926922\n",
      "Epoch 80, loss: 627.296064\n",
      "Epoch 81, loss: 616.827631\n",
      "Epoch 82, loss: 629.510811\n",
      "Epoch 83, loss: 625.244487\n",
      "Epoch 84, loss: 633.693793\n",
      "Epoch 85, loss: 628.234728\n",
      "Epoch 86, loss: 620.055336\n",
      "Epoch 87, loss: 623.662855\n",
      "Epoch 88, loss: 617.943660\n",
      "Epoch 89, loss: 620.618401\n",
      "Epoch 90, loss: 624.610091\n",
      "Epoch 91, loss: 634.754095\n",
      "Epoch 92, loss: 653.699116\n",
      "Epoch 93, loss: 608.135706\n",
      "Epoch 94, loss: 626.280652\n",
      "Epoch 95, loss: 638.805532\n",
      "Epoch 96, loss: 627.524930\n",
      "Epoch 97, loss: 623.469805\n",
      "Epoch 98, loss: 630.398015\n",
      "Epoch 99, loss: 625.565109\n",
      "Epoch 100, loss: 637.896933\n",
      "Epoch 101, loss: 621.554712\n",
      "Epoch 102, loss: 639.651145\n",
      "Epoch 103, loss: 634.026169\n",
      "Epoch 104, loss: 630.725233\n",
      "Epoch 105, loss: 628.026130\n",
      "Epoch 106, loss: 614.089929\n",
      "Epoch 107, loss: 618.167665\n",
      "Epoch 108, loss: 624.809094\n",
      "Epoch 109, loss: 620.538033\n",
      "Epoch 110, loss: 597.913209\n",
      "Epoch 111, loss: 638.502218\n",
      "Epoch 112, loss: 617.501028\n",
      "Epoch 113, loss: 621.484222\n",
      "Epoch 114, loss: 630.052770\n",
      "Epoch 115, loss: 635.479899\n",
      "Epoch 116, loss: 629.400243\n",
      "Epoch 117, loss: 620.344772\n",
      "Epoch 118, loss: 623.762615\n",
      "Epoch 119, loss: 612.653902\n",
      "Epoch 120, loss: 616.783733\n",
      "Epoch 121, loss: 627.008913\n",
      "Epoch 122, loss: 628.442424\n",
      "Epoch 123, loss: 637.650131\n",
      "Epoch 124, loss: 637.230122\n",
      "Epoch 125, loss: 626.491568\n",
      "Epoch 126, loss: 614.604115\n",
      "Epoch 127, loss: 637.224659\n",
      "Epoch 128, loss: 635.412457\n",
      "Epoch 129, loss: 613.519472\n",
      "Epoch 130, loss: 625.900279\n",
      "Epoch 131, loss: 599.777934\n",
      "Epoch 132, loss: 618.360266\n",
      "Epoch 133, loss: 623.490921\n",
      "Epoch 134, loss: 629.977708\n",
      "Epoch 135, loss: 633.241585\n",
      "Epoch 136, loss: 625.540825\n",
      "Epoch 137, loss: 612.000888\n",
      "Epoch 138, loss: 617.027329\n",
      "Epoch 139, loss: 625.677768\n",
      "Epoch 140, loss: 628.205155\n",
      "Epoch 141, loss: 619.150338\n",
      "Epoch 142, loss: 594.911410\n",
      "Epoch 143, loss: 608.235884\n",
      "Epoch 144, loss: 627.376551\n",
      "Epoch 145, loss: 638.043891\n",
      "Epoch 146, loss: 633.512408\n",
      "Epoch 147, loss: 614.433052\n",
      "Epoch 148, loss: 608.083868\n",
      "Epoch 149, loss: 611.065837\n",
      "Epoch 150, loss: 615.461388\n",
      "Epoch 151, loss: 617.907353\n",
      "Epoch 152, loss: 604.156887\n",
      "Epoch 153, loss: 615.259885\n",
      "Epoch 154, loss: 617.007039\n",
      "Epoch 155, loss: 622.658799\n",
      "Epoch 156, loss: 624.256208\n",
      "Epoch 157, loss: 617.316340\n",
      "Epoch 158, loss: 619.533051\n",
      "Epoch 159, loss: 638.338487\n",
      "Epoch 160, loss: 629.646393\n",
      "Epoch 161, loss: 627.668313\n",
      "Epoch 162, loss: 612.678355\n",
      "Epoch 163, loss: 626.320459\n",
      "Epoch 164, loss: 633.612440\n",
      "Epoch 165, loss: 614.200437\n",
      "Epoch 166, loss: 642.007794\n",
      "Epoch 167, loss: 632.862828\n",
      "Epoch 168, loss: 627.622565\n",
      "Epoch 169, loss: 643.388851\n",
      "Epoch 170, loss: 630.872487\n",
      "Epoch 171, loss: 618.846536\n",
      "Epoch 172, loss: 610.232126\n",
      "Epoch 173, loss: 630.847660\n",
      "Epoch 174, loss: 623.923543\n",
      "Epoch 175, loss: 608.230477\n",
      "Epoch 176, loss: 625.534539\n",
      "Epoch 177, loss: 616.442077\n",
      "Epoch 178, loss: 605.396398\n",
      "Epoch 179, loss: 605.418519\n",
      "Epoch 180, loss: 604.898560\n",
      "Epoch 181, loss: 605.950072\n",
      "Epoch 182, loss: 605.509251\n",
      "Epoch 183, loss: 632.221501\n",
      "Epoch 184, loss: 644.515713\n",
      "Epoch 185, loss: 622.105302\n",
      "Epoch 186, loss: 618.592346\n",
      "Epoch 187, loss: 627.701116\n",
      "Epoch 188, loss: 622.823531\n",
      "Epoch 189, loss: 636.243835\n",
      "Epoch 190, loss: 613.619086\n",
      "Epoch 191, loss: 628.436417\n",
      "Epoch 192, loss: 609.363651\n",
      "Epoch 193, loss: 609.537344\n",
      "Epoch 194, loss: 621.388935\n",
      "Epoch 195, loss: 622.899929\n",
      "Epoch 196, loss: 613.118973\n",
      "Epoch 197, loss: 615.574334\n",
      "Epoch 198, loss: 624.850195\n",
      "Epoch 199, loss: 624.634812\n",
      "Epoch 0, loss: 686.253847\n",
      "Epoch 1, loss: 680.222143\n",
      "Epoch 2, loss: 675.740719\n",
      "Epoch 3, loss: 673.459577\n",
      "Epoch 4, loss: 669.648589\n",
      "Epoch 5, loss: 663.484253\n",
      "Epoch 6, loss: 671.466984\n",
      "Epoch 7, loss: 664.394137\n",
      "Epoch 8, loss: 653.554214\n",
      "Epoch 9, loss: 655.479165\n",
      "Epoch 10, loss: 647.893957\n",
      "Epoch 11, loss: 647.759404\n",
      "Epoch 12, loss: 650.846041\n",
      "Epoch 13, loss: 643.718374\n",
      "Epoch 14, loss: 642.208524\n",
      "Epoch 15, loss: 654.346577\n",
      "Epoch 16, loss: 647.964453\n",
      "Epoch 17, loss: 636.542791\n",
      "Epoch 18, loss: 641.393672\n",
      "Epoch 19, loss: 644.301795\n",
      "Epoch 20, loss: 643.074825\n",
      "Epoch 21, loss: 647.692207\n",
      "Epoch 22, loss: 656.884085\n",
      "Epoch 23, loss: 643.095891\n",
      "Epoch 24, loss: 648.676466\n",
      "Epoch 25, loss: 638.879534\n",
      "Epoch 26, loss: 632.742211\n",
      "Epoch 27, loss: 645.716430\n",
      "Epoch 28, loss: 644.967168\n",
      "Epoch 29, loss: 639.364163\n",
      "Epoch 30, loss: 622.740276\n",
      "Epoch 31, loss: 649.600736\n",
      "Epoch 32, loss: 645.839911\n",
      "Epoch 33, loss: 632.332363\n",
      "Epoch 34, loss: 639.327581\n",
      "Epoch 35, loss: 656.298049\n",
      "Epoch 36, loss: 655.102754\n",
      "Epoch 37, loss: 632.079048\n",
      "Epoch 38, loss: 646.088564\n",
      "Epoch 39, loss: 634.361794\n",
      "Epoch 40, loss: 644.229312\n",
      "Epoch 41, loss: 630.786142\n",
      "Epoch 42, loss: 642.019175\n",
      "Epoch 43, loss: 628.542701\n",
      "Epoch 44, loss: 639.314726\n",
      "Epoch 45, loss: 643.461858\n",
      "Epoch 46, loss: 634.985730\n",
      "Epoch 47, loss: 635.011322\n",
      "Epoch 48, loss: 641.935427\n",
      "Epoch 49, loss: 637.327162\n",
      "Epoch 50, loss: 631.929421\n",
      "Epoch 51, loss: 640.251213\n",
      "Epoch 52, loss: 636.915020\n",
      "Epoch 53, loss: 634.286804\n",
      "Epoch 54, loss: 631.975452\n",
      "Epoch 55, loss: 641.725457\n",
      "Epoch 56, loss: 634.709883\n",
      "Epoch 57, loss: 617.072699\n",
      "Epoch 58, loss: 637.837922\n",
      "Epoch 59, loss: 636.111867\n",
      "Epoch 60, loss: 644.499579\n",
      "Epoch 61, loss: 638.088144\n",
      "Epoch 62, loss: 635.443752\n",
      "Epoch 63, loss: 620.299391\n",
      "Epoch 64, loss: 618.239575\n",
      "Epoch 65, loss: 612.793419\n",
      "Epoch 66, loss: 640.475619\n",
      "Epoch 67, loss: 625.973670\n",
      "Epoch 68, loss: 640.434549\n",
      "Epoch 69, loss: 622.985667\n",
      "Epoch 70, loss: 639.728252\n",
      "Epoch 71, loss: 622.101121\n",
      "Epoch 72, loss: 638.027606\n",
      "Epoch 73, loss: 641.222482\n",
      "Epoch 74, loss: 630.278150\n",
      "Epoch 75, loss: 637.698297\n",
      "Epoch 76, loss: 635.890533\n",
      "Epoch 77, loss: 631.025001\n",
      "Epoch 78, loss: 625.231206\n",
      "Epoch 79, loss: 620.386003\n",
      "Epoch 80, loss: 628.789165\n",
      "Epoch 81, loss: 616.140778\n",
      "Epoch 82, loss: 608.337295\n",
      "Epoch 83, loss: 628.131448\n",
      "Epoch 84, loss: 633.189621\n",
      "Epoch 85, loss: 628.286513\n",
      "Epoch 86, loss: 619.582415\n",
      "Epoch 87, loss: 638.253814\n",
      "Epoch 88, loss: 639.120797\n",
      "Epoch 89, loss: 628.031967\n",
      "Epoch 90, loss: 631.483076\n",
      "Epoch 91, loss: 624.908766\n",
      "Epoch 92, loss: 638.528942\n",
      "Epoch 93, loss: 619.949205\n",
      "Epoch 94, loss: 631.578557\n",
      "Epoch 95, loss: 639.732776\n",
      "Epoch 96, loss: 598.230423\n",
      "Epoch 97, loss: 628.513546\n",
      "Epoch 98, loss: 619.641524\n",
      "Epoch 99, loss: 620.557319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, loss: 645.117222\n",
      "Epoch 101, loss: 594.870496\n",
      "Epoch 102, loss: 622.832035\n",
      "Epoch 103, loss: 638.081989\n",
      "Epoch 104, loss: 616.658132\n",
      "Epoch 105, loss: 655.150327\n",
      "Epoch 106, loss: 621.668051\n",
      "Epoch 107, loss: 619.134235\n",
      "Epoch 108, loss: 630.653408\n",
      "Epoch 109, loss: 611.328579\n",
      "Epoch 110, loss: 619.170305\n",
      "Epoch 111, loss: 629.842669\n",
      "Epoch 112, loss: 618.029925\n",
      "Epoch 113, loss: 631.497948\n",
      "Epoch 114, loss: 603.855640\n",
      "Epoch 115, loss: 618.363221\n",
      "Epoch 116, loss: 621.060774\n",
      "Epoch 117, loss: 626.605631\n",
      "Epoch 118, loss: 635.780550\n",
      "Epoch 119, loss: 618.470663\n",
      "Epoch 120, loss: 626.005735\n",
      "Epoch 121, loss: 623.372079\n",
      "Epoch 122, loss: 608.727957\n",
      "Epoch 123, loss: 624.127419\n",
      "Epoch 124, loss: 638.606996\n",
      "Epoch 125, loss: 631.606588\n",
      "Epoch 126, loss: 605.390833\n",
      "Epoch 127, loss: 626.504131\n",
      "Epoch 128, loss: 604.456820\n",
      "Epoch 129, loss: 629.365139\n",
      "Epoch 130, loss: 635.267010\n",
      "Epoch 131, loss: 625.728660\n",
      "Epoch 132, loss: 622.343342\n",
      "Epoch 133, loss: 629.429444\n",
      "Epoch 134, loss: 611.428155\n",
      "Epoch 135, loss: 626.613087\n",
      "Epoch 136, loss: 639.453990\n",
      "Epoch 137, loss: 599.817426\n",
      "Epoch 138, loss: 620.846504\n",
      "Epoch 139, loss: 618.961996\n",
      "Epoch 140, loss: 624.069704\n",
      "Epoch 141, loss: 623.736300\n",
      "Epoch 142, loss: 625.000222\n",
      "Epoch 143, loss: 622.148982\n",
      "Epoch 144, loss: 647.925226\n",
      "Epoch 145, loss: 628.023824\n",
      "Epoch 146, loss: 640.618166\n",
      "Epoch 147, loss: 611.886672\n",
      "Epoch 148, loss: 614.481399\n",
      "Epoch 149, loss: 622.839715\n",
      "Epoch 150, loss: 615.065759\n",
      "Epoch 151, loss: 623.923563\n",
      "Epoch 152, loss: 623.399122\n",
      "Epoch 153, loss: 626.622802\n",
      "Epoch 154, loss: 631.208971\n",
      "Epoch 155, loss: 619.189042\n",
      "Epoch 156, loss: 620.173686\n",
      "Epoch 157, loss: 629.508811\n",
      "Epoch 158, loss: 591.007954\n",
      "Epoch 159, loss: 625.515232\n",
      "Epoch 160, loss: 616.870400\n",
      "Epoch 161, loss: 622.832454\n",
      "Epoch 162, loss: 613.746732\n",
      "Epoch 163, loss: 626.342265\n",
      "Epoch 164, loss: 617.675012\n",
      "Epoch 165, loss: 613.400373\n",
      "Epoch 166, loss: 618.140141\n",
      "Epoch 167, loss: 618.452929\n",
      "Epoch 168, loss: 634.169714\n",
      "Epoch 169, loss: 616.548165\n",
      "Epoch 170, loss: 610.923426\n",
      "Epoch 171, loss: 604.075799\n",
      "Epoch 172, loss: 633.266527\n",
      "Epoch 173, loss: 602.729562\n",
      "Epoch 174, loss: 625.763793\n",
      "Epoch 175, loss: 621.820740\n",
      "Epoch 176, loss: 610.213807\n",
      "Epoch 177, loss: 617.393634\n",
      "Epoch 178, loss: 620.681540\n",
      "Epoch 179, loss: 605.927084\n",
      "Epoch 180, loss: 620.084742\n",
      "Epoch 181, loss: 623.818737\n",
      "Epoch 182, loss: 612.456841\n",
      "Epoch 183, loss: 593.295513\n",
      "Epoch 184, loss: 615.481809\n",
      "Epoch 185, loss: 619.957391\n",
      "Epoch 186, loss: 619.379682\n",
      "Epoch 187, loss: 624.541522\n",
      "Epoch 188, loss: 617.952094\n",
      "Epoch 189, loss: 609.965218\n",
      "Epoch 190, loss: 618.782873\n",
      "Epoch 191, loss: 618.704806\n",
      "Epoch 192, loss: 603.271580\n",
      "Epoch 193, loss: 608.211852\n",
      "Epoch 194, loss: 608.299156\n",
      "Epoch 195, loss: 635.606118\n",
      "Epoch 196, loss: 657.779056\n",
      "Epoch 197, loss: 619.478784\n",
      "Epoch 198, loss: 607.674140\n",
      "Epoch 199, loss: 619.010209\n",
      "Epoch 0, loss: 685.935926\n",
      "Epoch 1, loss: 680.635126\n",
      "Epoch 2, loss: 673.127384\n",
      "Epoch 3, loss: 677.815348\n",
      "Epoch 4, loss: 669.261998\n",
      "Epoch 5, loss: 666.211310\n",
      "Epoch 6, loss: 665.208441\n",
      "Epoch 7, loss: 659.113723\n",
      "Epoch 8, loss: 664.510404\n",
      "Epoch 9, loss: 650.769451\n",
      "Epoch 10, loss: 658.788899\n",
      "Epoch 11, loss: 661.133550\n",
      "Epoch 12, loss: 652.040903\n",
      "Epoch 13, loss: 646.911314\n",
      "Epoch 14, loss: 656.592283\n",
      "Epoch 15, loss: 648.431359\n",
      "Epoch 16, loss: 650.455383\n",
      "Epoch 17, loss: 658.170396\n",
      "Epoch 18, loss: 647.889296\n",
      "Epoch 19, loss: 639.600529\n",
      "Epoch 20, loss: 639.931894\n",
      "Epoch 21, loss: 636.140635\n",
      "Epoch 22, loss: 649.748239\n",
      "Epoch 23, loss: 643.199619\n",
      "Epoch 24, loss: 626.386163\n",
      "Epoch 25, loss: 638.564785\n",
      "Epoch 26, loss: 647.301610\n",
      "Epoch 27, loss: 656.516955\n",
      "Epoch 28, loss: 638.014222\n",
      "Epoch 29, loss: 638.940565\n",
      "Epoch 30, loss: 630.104075\n",
      "Epoch 31, loss: 637.480007\n",
      "Epoch 32, loss: 651.678597\n",
      "Epoch 33, loss: 631.241873\n",
      "Epoch 34, loss: 652.168641\n",
      "Epoch 35, loss: 621.578107\n",
      "Epoch 36, loss: 646.050865\n",
      "Epoch 37, loss: 631.829647\n",
      "Epoch 38, loss: 628.996753\n",
      "Epoch 39, loss: 637.709916\n",
      "Epoch 40, loss: 630.501710\n",
      "Epoch 41, loss: 618.832955\n",
      "Epoch 42, loss: 627.886229\n",
      "Epoch 43, loss: 632.114725\n",
      "Epoch 44, loss: 638.122090\n",
      "Epoch 45, loss: 632.998739\n",
      "Epoch 46, loss: 654.770244\n",
      "Epoch 47, loss: 629.130455\n",
      "Epoch 48, loss: 654.719440\n",
      "Epoch 49, loss: 640.030651\n",
      "Epoch 50, loss: 637.386917\n",
      "Epoch 51, loss: 639.126568\n",
      "Epoch 52, loss: 636.707017\n",
      "Epoch 53, loss: 630.172891\n",
      "Epoch 54, loss: 624.872686\n",
      "Epoch 55, loss: 633.787771\n",
      "Epoch 56, loss: 615.589861\n",
      "Epoch 57, loss: 638.885497\n",
      "Epoch 58, loss: 647.258123\n",
      "Epoch 59, loss: 633.443326\n",
      "Epoch 60, loss: 611.454327\n",
      "Epoch 61, loss: 634.712914\n",
      "Epoch 62, loss: 636.989538\n",
      "Epoch 63, loss: 635.341308\n",
      "Epoch 64, loss: 629.277123\n",
      "Epoch 65, loss: 644.204498\n",
      "Epoch 66, loss: 626.676518\n",
      "Epoch 67, loss: 621.336454\n",
      "Epoch 68, loss: 615.978457\n",
      "Epoch 69, loss: 632.450390\n",
      "Epoch 70, loss: 627.544269\n",
      "Epoch 71, loss: 642.601044\n",
      "Epoch 72, loss: 630.748806\n",
      "Epoch 73, loss: 651.738595\n",
      "Epoch 74, loss: 628.202399\n",
      "Epoch 75, loss: 642.991586\n",
      "Epoch 76, loss: 638.157787\n",
      "Epoch 77, loss: 623.741782\n",
      "Epoch 78, loss: 627.158979\n",
      "Epoch 79, loss: 626.356639\n",
      "Epoch 80, loss: 628.443457\n",
      "Epoch 81, loss: 634.254259\n",
      "Epoch 82, loss: 627.606247\n",
      "Epoch 83, loss: 642.219789\n",
      "Epoch 84, loss: 638.842576\n",
      "Epoch 85, loss: 652.509362\n",
      "Epoch 86, loss: 619.351643\n",
      "Epoch 87, loss: 627.405761\n",
      "Epoch 88, loss: 623.420287\n",
      "Epoch 89, loss: 615.479503\n",
      "Epoch 90, loss: 645.697460\n",
      "Epoch 91, loss: 623.970801\n",
      "Epoch 92, loss: 637.963184\n",
      "Epoch 93, loss: 607.746678\n",
      "Epoch 94, loss: 617.484819\n",
      "Epoch 95, loss: 647.598278\n",
      "Epoch 96, loss: 641.063886\n",
      "Epoch 97, loss: 637.056588\n",
      "Epoch 98, loss: 630.917150\n",
      "Epoch 99, loss: 618.321101\n",
      "Epoch 100, loss: 625.916074\n",
      "Epoch 101, loss: 616.160933\n",
      "Epoch 102, loss: 613.948842\n",
      "Epoch 103, loss: 625.019656\n",
      "Epoch 104, loss: 643.314024\n",
      "Epoch 105, loss: 609.040662\n",
      "Epoch 106, loss: 627.915664\n",
      "Epoch 107, loss: 636.497459\n",
      "Epoch 108, loss: 625.562631\n",
      "Epoch 109, loss: 612.797870\n",
      "Epoch 110, loss: 626.610768\n",
      "Epoch 111, loss: 627.270025\n",
      "Epoch 112, loss: 630.984882\n",
      "Epoch 113, loss: 623.585416\n",
      "Epoch 114, loss: 621.698314\n",
      "Epoch 115, loss: 633.149626\n",
      "Epoch 116, loss: 610.511852\n",
      "Epoch 117, loss: 617.931100\n",
      "Epoch 118, loss: 629.422974\n",
      "Epoch 119, loss: 628.210547\n",
      "Epoch 120, loss: 636.558997\n",
      "Epoch 121, loss: 628.896788\n",
      "Epoch 122, loss: 606.702997\n",
      "Epoch 123, loss: 617.005873\n",
      "Epoch 124, loss: 619.843278\n",
      "Epoch 125, loss: 633.275471\n",
      "Epoch 126, loss: 623.896325\n",
      "Epoch 127, loss: 623.016161\n",
      "Epoch 128, loss: 627.201392\n",
      "Epoch 129, loss: 613.053809\n",
      "Epoch 130, loss: 609.081058\n",
      "Epoch 131, loss: 621.728803\n",
      "Epoch 132, loss: 626.062169\n",
      "Epoch 133, loss: 621.404974\n",
      "Epoch 134, loss: 635.569822\n",
      "Epoch 135, loss: 631.720003\n",
      "Epoch 136, loss: 622.656623\n",
      "Epoch 137, loss: 622.358610\n",
      "Epoch 138, loss: 618.387397\n",
      "Epoch 139, loss: 622.254167\n",
      "Epoch 140, loss: 625.625591\n",
      "Epoch 141, loss: 635.279383\n",
      "Epoch 142, loss: 610.462205\n",
      "Epoch 143, loss: 628.699215\n",
      "Epoch 144, loss: 631.091820\n",
      "Epoch 145, loss: 624.725952\n",
      "Epoch 146, loss: 634.358891\n",
      "Epoch 147, loss: 613.512700\n",
      "Epoch 148, loss: 624.713995\n",
      "Epoch 149, loss: 615.718140\n",
      "Epoch 150, loss: 628.801477\n",
      "Epoch 151, loss: 610.806281\n",
      "Epoch 152, loss: 618.457580\n",
      "Epoch 153, loss: 625.757145\n",
      "Epoch 154, loss: 616.447527\n",
      "Epoch 155, loss: 623.178122\n",
      "Epoch 156, loss: 632.759483\n",
      "Epoch 157, loss: 624.579764\n",
      "Epoch 158, loss: 622.970658\n",
      "Epoch 159, loss: 603.134889\n",
      "Epoch 160, loss: 629.724601\n",
      "Epoch 161, loss: 625.224211\n",
      "Epoch 162, loss: 629.115325\n",
      "Epoch 163, loss: 619.406344\n",
      "Epoch 164, loss: 620.559265\n",
      "Epoch 165, loss: 621.784649\n",
      "Epoch 166, loss: 628.507557\n",
      "Epoch 167, loss: 612.023915\n",
      "Epoch 168, loss: 619.933400\n",
      "Epoch 169, loss: 601.663101\n",
      "Epoch 170, loss: 632.314786\n",
      "Epoch 171, loss: 613.991379\n",
      "Epoch 172, loss: 621.908200\n",
      "Epoch 173, loss: 616.903464\n",
      "Epoch 174, loss: 632.555796\n",
      "Epoch 175, loss: 615.037527\n",
      "Epoch 176, loss: 607.036636\n",
      "Epoch 177, loss: 603.555500\n",
      "Epoch 178, loss: 624.683931\n",
      "Epoch 179, loss: 622.396311\n",
      "Epoch 180, loss: 604.568062\n",
      "Epoch 181, loss: 612.304669\n",
      "Epoch 182, loss: 620.284920\n",
      "Epoch 183, loss: 621.271194\n",
      "Epoch 184, loss: 618.947028\n",
      "Epoch 185, loss: 627.020369\n",
      "Epoch 186, loss: 633.349611\n",
      "Epoch 187, loss: 622.401240\n",
      "Epoch 188, loss: 624.099193\n",
      "Epoch 189, loss: 608.176377\n",
      "Epoch 190, loss: 641.126480\n",
      "Epoch 191, loss: 606.045703\n",
      "Epoch 192, loss: 629.149728\n",
      "Epoch 193, loss: 604.601944\n",
      "Epoch 194, loss: 608.048705\n",
      "Epoch 195, loss: 615.113326\n",
      "Epoch 196, loss: 620.594154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197, loss: 603.251252\n",
      "Epoch 198, loss: 631.075101\n",
      "Epoch 199, loss: 608.342506\n",
      "Epoch 0, loss: 690.362222\n",
      "Epoch 1, loss: 688.978660\n",
      "Epoch 2, loss: 688.114982\n",
      "Epoch 3, loss: 687.514888\n",
      "Epoch 4, loss: 688.148338\n",
      "Epoch 5, loss: 687.988318\n",
      "Epoch 6, loss: 684.074371\n",
      "Epoch 7, loss: 685.999052\n",
      "Epoch 8, loss: 685.447581\n",
      "Epoch 9, loss: 683.321261\n",
      "Epoch 10, loss: 681.958398\n",
      "Epoch 11, loss: 682.473587\n",
      "Epoch 12, loss: 683.254499\n",
      "Epoch 13, loss: 682.130550\n",
      "Epoch 14, loss: 682.097420\n",
      "Epoch 15, loss: 680.427152\n",
      "Epoch 16, loss: 677.151321\n",
      "Epoch 17, loss: 677.234825\n",
      "Epoch 18, loss: 678.274245\n",
      "Epoch 19, loss: 677.157331\n",
      "Epoch 20, loss: 674.489356\n",
      "Epoch 21, loss: 675.535336\n",
      "Epoch 22, loss: 674.067145\n",
      "Epoch 23, loss: 676.133523\n",
      "Epoch 24, loss: 674.326114\n",
      "Epoch 25, loss: 676.652742\n",
      "Epoch 26, loss: 676.303232\n",
      "Epoch 27, loss: 671.451010\n",
      "Epoch 28, loss: 674.700883\n",
      "Epoch 29, loss: 670.779883\n",
      "Epoch 30, loss: 677.288669\n",
      "Epoch 31, loss: 674.838693\n",
      "Epoch 32, loss: 677.649820\n",
      "Epoch 33, loss: 673.782631\n",
      "Epoch 34, loss: 668.096124\n",
      "Epoch 35, loss: 668.935817\n",
      "Epoch 36, loss: 670.536462\n",
      "Epoch 37, loss: 666.632669\n",
      "Epoch 38, loss: 664.251999\n",
      "Epoch 39, loss: 670.291815\n",
      "Epoch 40, loss: 670.697078\n",
      "Epoch 41, loss: 669.369117\n",
      "Epoch 42, loss: 668.081008\n",
      "Epoch 43, loss: 671.525012\n",
      "Epoch 44, loss: 664.618715\n",
      "Epoch 45, loss: 664.624386\n",
      "Epoch 46, loss: 659.724900\n",
      "Epoch 47, loss: 675.135216\n",
      "Epoch 48, loss: 666.696755\n",
      "Epoch 49, loss: 666.089991\n",
      "Epoch 50, loss: 666.061896\n",
      "Epoch 51, loss: 663.536803\n",
      "Epoch 52, loss: 665.235208\n",
      "Epoch 53, loss: 667.966338\n",
      "Epoch 54, loss: 666.170414\n",
      "Epoch 55, loss: 667.971565\n",
      "Epoch 56, loss: 669.476211\n",
      "Epoch 57, loss: 660.290270\n",
      "Epoch 58, loss: 661.111182\n",
      "Epoch 59, loss: 662.079450\n",
      "Epoch 60, loss: 664.893974\n",
      "Epoch 61, loss: 657.767459\n",
      "Epoch 62, loss: 666.881480\n",
      "Epoch 63, loss: 662.744573\n",
      "Epoch 64, loss: 659.850997\n",
      "Epoch 65, loss: 663.953742\n",
      "Epoch 66, loss: 659.786249\n",
      "Epoch 67, loss: 670.676969\n",
      "Epoch 68, loss: 658.746807\n",
      "Epoch 69, loss: 662.601181\n",
      "Epoch 70, loss: 666.682970\n",
      "Epoch 71, loss: 662.710500\n",
      "Epoch 72, loss: 655.680931\n",
      "Epoch 73, loss: 656.387372\n",
      "Epoch 74, loss: 651.558459\n",
      "Epoch 75, loss: 654.893651\n",
      "Epoch 76, loss: 662.691487\n",
      "Epoch 77, loss: 666.473903\n",
      "Epoch 78, loss: 658.313632\n",
      "Epoch 79, loss: 658.278944\n",
      "Epoch 80, loss: 658.867864\n",
      "Epoch 81, loss: 653.494287\n",
      "Epoch 82, loss: 662.400010\n",
      "Epoch 83, loss: 659.694037\n",
      "Epoch 84, loss: 649.097266\n",
      "Epoch 85, loss: 647.357281\n",
      "Epoch 86, loss: 661.734445\n",
      "Epoch 87, loss: 658.243439\n",
      "Epoch 88, loss: 655.723971\n",
      "Epoch 89, loss: 659.153966\n",
      "Epoch 90, loss: 642.190020\n",
      "Epoch 91, loss: 646.399966\n",
      "Epoch 92, loss: 649.719914\n",
      "Epoch 93, loss: 654.327244\n",
      "Epoch 94, loss: 660.907411\n",
      "Epoch 95, loss: 654.850688\n",
      "Epoch 96, loss: 654.435745\n",
      "Epoch 97, loss: 655.588664\n",
      "Epoch 98, loss: 647.980040\n",
      "Epoch 99, loss: 663.331456\n",
      "Epoch 100, loss: 654.751930\n",
      "Epoch 101, loss: 652.585989\n",
      "Epoch 102, loss: 649.781794\n",
      "Epoch 103, loss: 662.634733\n",
      "Epoch 104, loss: 654.993488\n",
      "Epoch 105, loss: 649.218624\n",
      "Epoch 106, loss: 650.012208\n",
      "Epoch 107, loss: 646.878661\n",
      "Epoch 108, loss: 655.915058\n",
      "Epoch 109, loss: 661.261172\n",
      "Epoch 110, loss: 645.103003\n",
      "Epoch 111, loss: 650.531624\n",
      "Epoch 112, loss: 654.686453\n",
      "Epoch 113, loss: 660.273594\n",
      "Epoch 114, loss: 651.210774\n",
      "Epoch 115, loss: 650.910159\n",
      "Epoch 116, loss: 653.677091\n",
      "Epoch 117, loss: 641.509317\n",
      "Epoch 118, loss: 643.663703\n",
      "Epoch 119, loss: 667.084464\n",
      "Epoch 120, loss: 652.363980\n",
      "Epoch 121, loss: 655.278369\n",
      "Epoch 122, loss: 660.046457\n",
      "Epoch 123, loss: 645.378031\n",
      "Epoch 124, loss: 650.419299\n",
      "Epoch 125, loss: 652.709812\n",
      "Epoch 126, loss: 638.092107\n",
      "Epoch 127, loss: 660.700180\n",
      "Epoch 128, loss: 648.893758\n",
      "Epoch 129, loss: 658.548647\n",
      "Epoch 130, loss: 669.811982\n",
      "Epoch 131, loss: 653.044935\n",
      "Epoch 132, loss: 647.481803\n",
      "Epoch 133, loss: 648.045408\n",
      "Epoch 134, loss: 644.314858\n",
      "Epoch 135, loss: 646.615253\n",
      "Epoch 136, loss: 647.316930\n",
      "Epoch 137, loss: 653.265819\n",
      "Epoch 138, loss: 644.946597\n",
      "Epoch 139, loss: 654.405636\n",
      "Epoch 140, loss: 644.607432\n",
      "Epoch 141, loss: 659.141145\n",
      "Epoch 142, loss: 651.771640\n",
      "Epoch 143, loss: 645.750441\n",
      "Epoch 144, loss: 648.659452\n",
      "Epoch 145, loss: 649.132239\n",
      "Epoch 146, loss: 647.971392\n",
      "Epoch 147, loss: 655.982767\n",
      "Epoch 148, loss: 658.994641\n",
      "Epoch 149, loss: 650.102212\n",
      "Epoch 150, loss: 639.791873\n",
      "Epoch 151, loss: 643.475843\n",
      "Epoch 152, loss: 640.490539\n",
      "Epoch 153, loss: 653.694325\n",
      "Epoch 154, loss: 640.588903\n",
      "Epoch 155, loss: 649.284649\n",
      "Epoch 156, loss: 645.505584\n",
      "Epoch 157, loss: 653.128422\n",
      "Epoch 158, loss: 658.493372\n",
      "Epoch 159, loss: 649.152391\n",
      "Epoch 160, loss: 636.259874\n",
      "Epoch 161, loss: 658.699930\n",
      "Epoch 162, loss: 646.470297\n",
      "Epoch 163, loss: 640.332084\n",
      "Epoch 164, loss: 642.883196\n",
      "Epoch 165, loss: 653.117304\n",
      "Epoch 166, loss: 662.983553\n",
      "Epoch 167, loss: 644.648199\n",
      "Epoch 168, loss: 648.220913\n",
      "Epoch 169, loss: 645.952232\n",
      "Epoch 170, loss: 636.147828\n",
      "Epoch 171, loss: 658.938267\n",
      "Epoch 172, loss: 642.457419\n",
      "Epoch 173, loss: 655.644615\n",
      "Epoch 174, loss: 648.838873\n",
      "Epoch 175, loss: 651.373771\n",
      "Epoch 176, loss: 651.863405\n",
      "Epoch 177, loss: 646.881004\n",
      "Epoch 178, loss: 636.569721\n",
      "Epoch 179, loss: 652.908493\n",
      "Epoch 180, loss: 642.957353\n",
      "Epoch 181, loss: 653.484678\n",
      "Epoch 182, loss: 649.542413\n",
      "Epoch 183, loss: 658.171599\n",
      "Epoch 184, loss: 643.868718\n",
      "Epoch 185, loss: 642.575377\n",
      "Epoch 186, loss: 640.338544\n",
      "Epoch 187, loss: 651.298242\n",
      "Epoch 188, loss: 641.105747\n",
      "Epoch 189, loss: 637.545700\n",
      "Epoch 190, loss: 640.533862\n",
      "Epoch 191, loss: 632.274616\n",
      "Epoch 192, loss: 644.728894\n",
      "Epoch 193, loss: 663.010996\n",
      "Epoch 194, loss: 644.949419\n",
      "Epoch 195, loss: 651.879591\n",
      "Epoch 196, loss: 646.564173\n",
      "Epoch 197, loss: 650.988365\n",
      "Epoch 198, loss: 645.929081\n",
      "Epoch 199, loss: 652.079373\n",
      "Epoch 0, loss: 690.234364\n",
      "Epoch 1, loss: 690.102874\n",
      "Epoch 2, loss: 689.099249\n",
      "Epoch 3, loss: 687.250699\n",
      "Epoch 4, loss: 686.581931\n",
      "Epoch 5, loss: 688.940125\n",
      "Epoch 6, loss: 685.712479\n",
      "Epoch 7, loss: 684.770959\n",
      "Epoch 8, loss: 685.549862\n",
      "Epoch 9, loss: 683.229422\n",
      "Epoch 10, loss: 682.140333\n",
      "Epoch 11, loss: 685.133694\n",
      "Epoch 12, loss: 683.039198\n",
      "Epoch 13, loss: 685.536439\n",
      "Epoch 14, loss: 679.774649\n",
      "Epoch 15, loss: 683.088799\n",
      "Epoch 16, loss: 676.959776\n",
      "Epoch 17, loss: 677.081980\n",
      "Epoch 18, loss: 678.306501\n",
      "Epoch 19, loss: 674.851808\n",
      "Epoch 20, loss: 676.814417\n",
      "Epoch 21, loss: 676.769667\n",
      "Epoch 22, loss: 675.786810\n",
      "Epoch 23, loss: 678.640282\n",
      "Epoch 24, loss: 672.771390\n",
      "Epoch 25, loss: 672.833336\n",
      "Epoch 26, loss: 671.584510\n",
      "Epoch 27, loss: 677.417649\n",
      "Epoch 28, loss: 673.946584\n",
      "Epoch 29, loss: 671.804117\n",
      "Epoch 30, loss: 669.725235\n",
      "Epoch 31, loss: 676.937238\n",
      "Epoch 32, loss: 675.736945\n",
      "Epoch 33, loss: 670.911565\n",
      "Epoch 34, loss: 673.595045\n",
      "Epoch 35, loss: 673.654236\n",
      "Epoch 36, loss: 675.793098\n",
      "Epoch 37, loss: 676.554936\n",
      "Epoch 38, loss: 667.394311\n",
      "Epoch 39, loss: 673.650460\n",
      "Epoch 40, loss: 664.963760\n",
      "Epoch 41, loss: 665.812233\n",
      "Epoch 42, loss: 668.377596\n",
      "Epoch 43, loss: 666.005459\n",
      "Epoch 44, loss: 663.693728\n",
      "Epoch 45, loss: 666.051699\n",
      "Epoch 46, loss: 660.751248\n",
      "Epoch 47, loss: 664.543882\n",
      "Epoch 48, loss: 664.494426\n",
      "Epoch 49, loss: 672.169605\n",
      "Epoch 50, loss: 660.798195\n",
      "Epoch 51, loss: 667.764784\n",
      "Epoch 52, loss: 663.461223\n",
      "Epoch 53, loss: 661.037749\n",
      "Epoch 54, loss: 668.884306\n",
      "Epoch 55, loss: 662.771196\n",
      "Epoch 56, loss: 660.922414\n",
      "Epoch 57, loss: 661.158485\n",
      "Epoch 58, loss: 657.088796\n",
      "Epoch 59, loss: 663.126674\n",
      "Epoch 60, loss: 658.767269\n",
      "Epoch 61, loss: 659.805441\n",
      "Epoch 62, loss: 658.657583\n",
      "Epoch 63, loss: 652.306794\n",
      "Epoch 64, loss: 662.187247\n",
      "Epoch 65, loss: 664.044747\n",
      "Epoch 66, loss: 658.212818\n",
      "Epoch 67, loss: 660.821220\n",
      "Epoch 68, loss: 664.403135\n",
      "Epoch 69, loss: 666.982550\n",
      "Epoch 70, loss: 661.415884\n",
      "Epoch 71, loss: 662.374354\n",
      "Epoch 72, loss: 655.385172\n",
      "Epoch 73, loss: 648.872313\n",
      "Epoch 74, loss: 650.716933\n",
      "Epoch 75, loss: 652.876737\n",
      "Epoch 76, loss: 662.003911\n",
      "Epoch 77, loss: 653.091276\n",
      "Epoch 78, loss: 647.247300\n",
      "Epoch 79, loss: 658.084280\n",
      "Epoch 80, loss: 658.436729\n",
      "Epoch 81, loss: 664.621822\n",
      "Epoch 82, loss: 658.989328\n",
      "Epoch 83, loss: 647.942046\n",
      "Epoch 84, loss: 652.734099\n",
      "Epoch 85, loss: 659.867725\n",
      "Epoch 86, loss: 666.035560\n",
      "Epoch 87, loss: 658.658733\n",
      "Epoch 88, loss: 656.716675\n",
      "Epoch 89, loss: 652.744381\n",
      "Epoch 90, loss: 652.504807\n",
      "Epoch 91, loss: 647.519139\n",
      "Epoch 92, loss: 654.345650\n",
      "Epoch 93, loss: 662.599055\n",
      "Epoch 94, loss: 666.340683\n",
      "Epoch 95, loss: 657.663225\n",
      "Epoch 96, loss: 656.186338\n",
      "Epoch 97, loss: 651.256019\n",
      "Epoch 98, loss: 654.471433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, loss: 639.655563\n",
      "Epoch 100, loss: 656.200755\n",
      "Epoch 101, loss: 654.854818\n",
      "Epoch 102, loss: 657.418293\n",
      "Epoch 103, loss: 656.793783\n",
      "Epoch 104, loss: 651.550868\n",
      "Epoch 105, loss: 647.663711\n",
      "Epoch 106, loss: 653.229907\n",
      "Epoch 107, loss: 646.843317\n",
      "Epoch 108, loss: 653.577154\n",
      "Epoch 109, loss: 660.947735\n",
      "Epoch 110, loss: 667.644671\n",
      "Epoch 111, loss: 650.912213\n",
      "Epoch 112, loss: 636.406861\n",
      "Epoch 113, loss: 649.376792\n",
      "Epoch 114, loss: 653.012925\n",
      "Epoch 115, loss: 641.253335\n",
      "Epoch 116, loss: 654.138924\n",
      "Epoch 117, loss: 644.179369\n",
      "Epoch 118, loss: 665.492092\n",
      "Epoch 119, loss: 648.941644\n",
      "Epoch 120, loss: 651.196984\n",
      "Epoch 121, loss: 651.155982\n",
      "Epoch 122, loss: 652.971078\n",
      "Epoch 123, loss: 647.497689\n",
      "Epoch 124, loss: 648.007691\n",
      "Epoch 125, loss: 654.185779\n",
      "Epoch 126, loss: 661.452213\n",
      "Epoch 127, loss: 651.048175\n",
      "Epoch 128, loss: 650.863527\n",
      "Epoch 129, loss: 660.665569\n",
      "Epoch 130, loss: 646.024435\n",
      "Epoch 131, loss: 659.978970\n",
      "Epoch 132, loss: 652.052243\n",
      "Epoch 133, loss: 650.086365\n",
      "Epoch 134, loss: 644.622384\n",
      "Epoch 135, loss: 645.334025\n",
      "Epoch 136, loss: 655.462824\n",
      "Epoch 137, loss: 666.586605\n",
      "Epoch 138, loss: 650.856981\n",
      "Epoch 139, loss: 648.315208\n",
      "Epoch 140, loss: 654.755680\n",
      "Epoch 141, loss: 647.641719\n",
      "Epoch 142, loss: 649.969632\n",
      "Epoch 143, loss: 646.451940\n",
      "Epoch 144, loss: 650.742273\n",
      "Epoch 145, loss: 643.200613\n",
      "Epoch 146, loss: 653.915038\n",
      "Epoch 147, loss: 647.129928\n",
      "Epoch 148, loss: 639.144258\n",
      "Epoch 149, loss: 656.881909\n",
      "Epoch 150, loss: 637.541869\n",
      "Epoch 151, loss: 648.961821\n",
      "Epoch 152, loss: 651.262390\n",
      "Epoch 153, loss: 649.196366\n",
      "Epoch 154, loss: 643.465035\n",
      "Epoch 155, loss: 655.236140\n",
      "Epoch 156, loss: 644.213487\n",
      "Epoch 157, loss: 653.555007\n",
      "Epoch 158, loss: 652.736679\n",
      "Epoch 159, loss: 642.280879\n",
      "Epoch 160, loss: 632.492837\n",
      "Epoch 161, loss: 648.451757\n",
      "Epoch 162, loss: 656.792735\n",
      "Epoch 163, loss: 655.086105\n",
      "Epoch 164, loss: 644.224516\n",
      "Epoch 165, loss: 633.237156\n",
      "Epoch 166, loss: 656.510523\n",
      "Epoch 167, loss: 639.438428\n",
      "Epoch 168, loss: 641.562912\n",
      "Epoch 169, loss: 646.736488\n",
      "Epoch 170, loss: 652.656284\n",
      "Epoch 171, loss: 638.466631\n",
      "Epoch 172, loss: 642.948064\n",
      "Epoch 173, loss: 641.718692\n",
      "Epoch 174, loss: 650.704562\n",
      "Epoch 175, loss: 652.302685\n",
      "Epoch 176, loss: 643.301023\n",
      "Epoch 177, loss: 654.093494\n",
      "Epoch 178, loss: 634.169991\n",
      "Epoch 179, loss: 648.663699\n",
      "Epoch 180, loss: 649.906553\n",
      "Epoch 181, loss: 650.699421\n",
      "Epoch 182, loss: 652.466047\n",
      "Epoch 183, loss: 634.093267\n",
      "Epoch 184, loss: 665.640959\n",
      "Epoch 185, loss: 644.218540\n",
      "Epoch 186, loss: 639.021265\n",
      "Epoch 187, loss: 641.265006\n",
      "Epoch 188, loss: 638.017688\n",
      "Epoch 189, loss: 653.521701\n",
      "Epoch 190, loss: 636.439631\n",
      "Epoch 191, loss: 646.587044\n",
      "Epoch 192, loss: 651.498350\n",
      "Epoch 193, loss: 646.197771\n",
      "Epoch 194, loss: 640.614330\n",
      "Epoch 195, loss: 642.158197\n",
      "Epoch 196, loss: 648.862381\n",
      "Epoch 197, loss: 646.685789\n",
      "Epoch 198, loss: 667.492409\n",
      "Epoch 199, loss: 640.749055\n",
      "Epoch 0, loss: 690.829392\n",
      "Epoch 1, loss: 689.090443\n",
      "Epoch 2, loss: 689.193219\n",
      "Epoch 3, loss: 688.404227\n",
      "Epoch 4, loss: 686.820948\n",
      "Epoch 5, loss: 686.439204\n",
      "Epoch 6, loss: 683.432231\n",
      "Epoch 7, loss: 684.530299\n",
      "Epoch 8, loss: 684.889992\n",
      "Epoch 9, loss: 682.828763\n",
      "Epoch 10, loss: 681.758121\n",
      "Epoch 11, loss: 684.821264\n",
      "Epoch 12, loss: 680.730848\n",
      "Epoch 13, loss: 680.078743\n",
      "Epoch 14, loss: 680.578245\n",
      "Epoch 15, loss: 679.758397\n",
      "Epoch 16, loss: 681.152696\n",
      "Epoch 17, loss: 678.929288\n",
      "Epoch 18, loss: 677.125753\n",
      "Epoch 19, loss: 678.129167\n",
      "Epoch 20, loss: 674.775601\n",
      "Epoch 21, loss: 676.556524\n",
      "Epoch 22, loss: 674.957854\n",
      "Epoch 23, loss: 680.953874\n",
      "Epoch 24, loss: 676.774333\n",
      "Epoch 25, loss: 673.479156\n",
      "Epoch 26, loss: 677.109579\n",
      "Epoch 27, loss: 674.564619\n",
      "Epoch 28, loss: 673.977613\n",
      "Epoch 29, loss: 669.051362\n",
      "Epoch 30, loss: 671.332218\n",
      "Epoch 31, loss: 676.039086\n",
      "Epoch 32, loss: 668.319843\n",
      "Epoch 33, loss: 672.002035\n",
      "Epoch 34, loss: 669.694414\n",
      "Epoch 35, loss: 672.414651\n",
      "Epoch 36, loss: 669.437480\n",
      "Epoch 37, loss: 670.439278\n",
      "Epoch 38, loss: 671.912663\n",
      "Epoch 39, loss: 671.830623\n",
      "Epoch 40, loss: 669.585274\n",
      "Epoch 41, loss: 674.888466\n",
      "Epoch 42, loss: 670.298331\n",
      "Epoch 43, loss: 667.597020\n",
      "Epoch 44, loss: 665.716047\n",
      "Epoch 45, loss: 664.418279\n",
      "Epoch 46, loss: 656.723952\n",
      "Epoch 47, loss: 663.393413\n",
      "Epoch 48, loss: 671.296146\n",
      "Epoch 49, loss: 667.921089\n",
      "Epoch 50, loss: 661.472949\n",
      "Epoch 51, loss: 659.829894\n",
      "Epoch 52, loss: 666.190541\n",
      "Epoch 53, loss: 658.741439\n",
      "Epoch 54, loss: 671.923621\n",
      "Epoch 55, loss: 663.587753\n",
      "Epoch 56, loss: 668.258619\n",
      "Epoch 57, loss: 670.193532\n",
      "Epoch 58, loss: 664.803763\n",
      "Epoch 59, loss: 662.079937\n",
      "Epoch 60, loss: 666.928607\n",
      "Epoch 61, loss: 668.564920\n",
      "Epoch 62, loss: 659.887917\n",
      "Epoch 63, loss: 662.934188\n",
      "Epoch 64, loss: 660.772288\n",
      "Epoch 65, loss: 656.178103\n",
      "Epoch 66, loss: 653.210125\n",
      "Epoch 67, loss: 665.803837\n",
      "Epoch 68, loss: 663.381856\n",
      "Epoch 69, loss: 661.158994\n",
      "Epoch 70, loss: 656.218922\n",
      "Epoch 71, loss: 668.287634\n",
      "Epoch 72, loss: 645.523468\n",
      "Epoch 73, loss: 661.896143\n",
      "Epoch 74, loss: 666.811559\n",
      "Epoch 75, loss: 661.414151\n",
      "Epoch 76, loss: 650.857558\n",
      "Epoch 77, loss: 661.357790\n",
      "Epoch 78, loss: 663.368501\n",
      "Epoch 79, loss: 649.270326\n",
      "Epoch 80, loss: 658.889117\n",
      "Epoch 81, loss: 660.231828\n",
      "Epoch 82, loss: 653.139833\n",
      "Epoch 83, loss: 653.372142\n",
      "Epoch 84, loss: 661.349044\n",
      "Epoch 85, loss: 654.845580\n",
      "Epoch 86, loss: 652.084476\n",
      "Epoch 87, loss: 664.889864\n",
      "Epoch 88, loss: 650.872412\n",
      "Epoch 89, loss: 661.720121\n",
      "Epoch 90, loss: 658.967321\n",
      "Epoch 91, loss: 664.957095\n",
      "Epoch 92, loss: 658.232593\n",
      "Epoch 93, loss: 656.859561\n",
      "Epoch 94, loss: 654.575721\n",
      "Epoch 95, loss: 659.896752\n",
      "Epoch 96, loss: 654.816512\n",
      "Epoch 97, loss: 638.354307\n",
      "Epoch 98, loss: 659.724620\n",
      "Epoch 99, loss: 653.837853\n",
      "Epoch 100, loss: 647.962090\n",
      "Epoch 101, loss: 649.645931\n",
      "Epoch 102, loss: 648.136533\n",
      "Epoch 103, loss: 655.828149\n",
      "Epoch 104, loss: 662.747539\n",
      "Epoch 105, loss: 656.343587\n",
      "Epoch 106, loss: 656.504883\n",
      "Epoch 107, loss: 655.452386\n",
      "Epoch 108, loss: 653.188135\n",
      "Epoch 109, loss: 650.420799\n",
      "Epoch 110, loss: 648.576631\n",
      "Epoch 111, loss: 646.734925\n",
      "Epoch 112, loss: 655.102051\n",
      "Epoch 113, loss: 652.527751\n",
      "Epoch 114, loss: 645.333756\n",
      "Epoch 115, loss: 646.908348\n",
      "Epoch 116, loss: 649.089052\n",
      "Epoch 117, loss: 657.431077\n",
      "Epoch 118, loss: 648.653671\n",
      "Epoch 119, loss: 651.759420\n",
      "Epoch 120, loss: 651.983116\n",
      "Epoch 121, loss: 659.296188\n",
      "Epoch 122, loss: 654.278160\n",
      "Epoch 123, loss: 654.146247\n",
      "Epoch 124, loss: 648.600341\n",
      "Epoch 125, loss: 650.816511\n",
      "Epoch 126, loss: 650.347596\n",
      "Epoch 127, loss: 644.085145\n",
      "Epoch 128, loss: 651.320697\n",
      "Epoch 129, loss: 645.329500\n",
      "Epoch 130, loss: 653.288821\n",
      "Epoch 131, loss: 650.073826\n",
      "Epoch 132, loss: 649.487260\n",
      "Epoch 133, loss: 654.333432\n",
      "Epoch 134, loss: 646.235197\n",
      "Epoch 135, loss: 656.785729\n",
      "Epoch 136, loss: 661.513182\n",
      "Epoch 137, loss: 657.420660\n",
      "Epoch 138, loss: 654.037738\n",
      "Epoch 139, loss: 653.894164\n",
      "Epoch 140, loss: 647.690852\n",
      "Epoch 141, loss: 653.816552\n",
      "Epoch 142, loss: 643.427717\n",
      "Epoch 143, loss: 646.255045\n",
      "Epoch 144, loss: 655.682122\n",
      "Epoch 145, loss: 645.071113\n",
      "Epoch 146, loss: 652.400554\n",
      "Epoch 147, loss: 644.938808\n",
      "Epoch 148, loss: 639.413535\n",
      "Epoch 149, loss: 639.237336\n",
      "Epoch 150, loss: 657.197917\n",
      "Epoch 151, loss: 640.471243\n",
      "Epoch 152, loss: 647.490966\n",
      "Epoch 153, loss: 648.458750\n",
      "Epoch 154, loss: 647.288707\n",
      "Epoch 155, loss: 648.709245\n",
      "Epoch 156, loss: 651.443482\n",
      "Epoch 157, loss: 654.982058\n",
      "Epoch 158, loss: 646.002655\n",
      "Epoch 159, loss: 648.167539\n",
      "Epoch 160, loss: 653.587266\n",
      "Epoch 161, loss: 639.966043\n",
      "Epoch 162, loss: 647.199269\n",
      "Epoch 163, loss: 628.191963\n",
      "Epoch 164, loss: 650.744836\n",
      "Epoch 165, loss: 647.924084\n",
      "Epoch 166, loss: 646.477970\n",
      "Epoch 167, loss: 634.052532\n",
      "Epoch 168, loss: 644.554687\n",
      "Epoch 169, loss: 640.511760\n",
      "Epoch 170, loss: 646.031309\n",
      "Epoch 171, loss: 644.893761\n",
      "Epoch 172, loss: 644.484168\n",
      "Epoch 173, loss: 644.630937\n",
      "Epoch 174, loss: 637.282302\n",
      "Epoch 175, loss: 652.446660\n",
      "Epoch 176, loss: 644.673675\n",
      "Epoch 177, loss: 648.027686\n",
      "Epoch 178, loss: 636.774978\n",
      "Epoch 179, loss: 653.663779\n",
      "Epoch 180, loss: 635.301874\n",
      "Epoch 181, loss: 643.604339\n",
      "Epoch 182, loss: 638.131735\n",
      "Epoch 183, loss: 649.234086\n",
      "Epoch 184, loss: 655.660932\n",
      "Epoch 185, loss: 634.886120\n",
      "Epoch 186, loss: 639.011090\n",
      "Epoch 187, loss: 644.099271\n",
      "Epoch 188, loss: 648.632611\n",
      "Epoch 189, loss: 650.104250\n",
      "Epoch 190, loss: 637.131487\n",
      "Epoch 191, loss: 643.870691\n",
      "Epoch 192, loss: 651.536135\n",
      "Epoch 193, loss: 654.346579\n",
      "Epoch 194, loss: 631.883694\n",
      "Epoch 195, loss: 639.900790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196, loss: 634.201615\n",
      "Epoch 197, loss: 653.656498\n",
      "Epoch 198, loss: 645.853381\n",
      "Epoch 199, loss: 653.357135\n",
      "best validation accuracy achieved: 0.210000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        clf = linear_classifer.LinearSoftmaxClassifier()\n",
    "        clf.fit(train_X, train_y, batch_size = batch_size, learning_rate = learning_rate, reg = reg, epochs = num_epochs)\n",
    "        preds = clf.predict(test_X)\n",
    "        accuracy = multiclass_accuracy(preds, test_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = clf\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.210000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
